---
title: "analysis"
author: "Eirini Zormpa"
date: "09/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
## install what you don't have
# devtools::install_github("benmarwick/rrtools")
# install.packages("here")
# install.packages("readr")
# install.packages("magrittr")
# install.packages("rio")
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages("lme4")
# remotes::install_github("karthik/holepunch")

library(rrtools)
library(here)
library(readr)
library(magrittr)
library(rio)
library(dplyr)
library(ggplot2)
library(lme4)
library(holepunch)
```

```{r add_license}
# usethis::use_mit_license(name = "Eirini Zormpa")
```


```{r add_readme}
# rrtools::use_readme_rmd()
```


```{r}
# write_compendium_description(package = "KLPS5", 
#                              description = "A demo of reproducible analysis for the Key Practices for the Languages course, session 5",
#                              path = here())
```

```{r}
# write_dockerfile(maintainer = "Eirini Zormpa")
```

```{r binder_badge}
# generate_badge()
```

# Prepare data

## Naming data

```{r read_annotations_local_wd}
# set working directory to project directory
setwd("C:/Users/eirin/Documents/PhD/Classes/KPLS2020/KPLS5")

# read in annotations
naming_onsets_local <- read_delim("Data/Raw/Naming/AH_NamingOnsets.txt", delim = "\t")

naming_onsets_local %<>%
  select(-offset) %>%
  rename(praat_label = label)
```

```{r read_naming_logfiles_local_here}
# create header
header = c("Subject","NamingTrial","ID","Probe_type","Stimulus_type","Response_type","File_name","English_name","Dutch_name","Other_A","Other_B","List", "Compound", "Agreement_percentage", "DK_percentage", "Lg10WF", "Familiarity", "Visual_complexity", "Manipulability", "Letters", "Non-word", "CorrectName", "Response")

# get names of all logfiles
filelist = list.files(path = here("Data", "Raw", "Naming", "Logfiles"), pattern = ".*.txt")

naming_logfiles_local <- read_delim(here("Data", "Raw", "Naming", "Logfiles", filelist[1]), delim="\t", col_names = header, locale = locale("nl"), skip = 1)

for (i in 2:length(filelist)){
  naming_logfiles_local2 <- read_delim(here("Data", "Raw", "Naming", "Logfiles", filelist[i]), delim="\t", col_names = header, locale = locale("nl"), skip = 1)
  naming_logfiles_local <- merge(naming_logfiles_local, naming_logfiles_local2, all = T)
}

rm(naming_logfiles_local2)
```

```{r export_naming_logfiles}
write_delim(naming_logfiles_local, here("Data", "Raw", "Naming", "Logfiles", "AH_naming_logfiles.txt"), delim = "\t")
```


```{r read_annotations_osf}
naming_onsets <- rio::import("https://osf.io/mvtra/download", "txt")

naming_onsets %<>%
  select(-offset) %>%
  rename(praat_label = label)

rm(naming_onsets_local)
```


```{r clean_annotations}
# Keep only the aloud items (42*64 = 2688) and get rid of unnecessary lines from praat annotation (time before and after word).
# The silent items have no onset and the two chunks with the condition and comments are not useful to me.
aloud_onsets <- filter(naming_onsets, Response_type == "aloud")  %>%
  filter(as.character(praat_label) == as.character(Dutch_name))

# Sanity check (same number of aloud trials from all participants)
sanity_check <- aloud_onsets %>%
  group_by(Subject) %>%
  summarise(no_rows = length(Subject))

sanity_check$no_rows
```

```{r read_naming_logfiles_osf}
naming <- rio::import("https://osf.io/3rv5f/download", "txt")
```

```{r clean_naming_data}
naming %<>% 
  select (Subject, ID, Probe_type, Stimulus_type, Response_type, English_name, Dutch_name, CorrectName) %>%
  arrange(Subject, Dutch_name)
```

```{r naming_accuracy}
nam_acc <- mean(naming$CorrectName)
  
#Check how accurate individual participants are with naming
NamAccuracy_cond <- summarise(group_by(naming, Subject, Stimulus_type, Response_type),
                              mean_correct = mean(CorrectName))

ggplot(NamAccuracy_cond)+
  geom_point(aes(x=mean_correct,y=paste(Response_type,Stimulus_type)))+
  facet_wrap(~Subject)
```

```{r join_naming_datasets}
# To join the timing onsets with the aloud items first separate aloud and silent items from naming data
aloud <- naming %>%
  filter(Response_type == "aloud")

silent <- naming %>%
  filter(Response_type == "silent")

# Filtering out incorrect trials
aloud_all <- inner_join(aloud_onsets, aloud, by = c("Subject", "Dutch_name", "Stimulus_type", "Response_type"))%>%
  select(-onset) %>%
  rename(onset = Real_onset) %>%
  mutate(onset = onset - 800)

aloud_correct <- aloud_all %>% filter(CorrectName == 1)
```

```{r visualise_naming_onsets_picture}
Picture_onsets_sub <- aloud_correct %>%
  filter(Stimulus_type == "picture") %>%
  group_by(Subject) %>%
  summarise(mean_RT=mean(onset)) %>%
  arrange(Subject)

ggplot() +
  geom_histogram(binwidth = 15, aes(x=mean_RT),data=Picture_onsets_sub)

Picture_onsets_trial <- aloud_correct %>%
  filter(Stimulus_type == "picture")

ggplot() +
  geom_histogram(binwidth = 20, aes(x=onset),data=Picture_onsets_trial)+
  facet_wrap(~Subject)
```

```{r check_outliers_picture}
# Participant 33 seems slower than the rest with a right-tailed distribution and is excluded
lat_33 <- aloud_correct %>% filter(Subject=="AH33")

mean_33 <- summarise(lat_33, mean_RT = mean(onset), sd_RT = sd(onset))

# exclude participant 33
aloud_correct %<>% filter(Subject != "AH33")
```

```{r picture_onsets_table}
Picture_onsets_descr <- aloud_correct %>%
  filter(Stimulus_type == "picture") %>%
  summarise(mean_RT = mean(onset), sd_RT = sd(onset))
```

```{r visualise_naming_onsets_picture_word}
PicWord_onsets_sub <- aloud_correct %>%
  filter(Stimulus_type == "picture+word") %>%
  group_by(Subject) %>%
  arrange(Subject)

ggplot() +
  geom_histogram(binwidth = 15, aes(x=onset),data=PicWord_onsets_sub)

PicWords_onsets_trial <- aloud_correct %>%
  filter(Stimulus_type == "picture+word")

ggplot() +
  geom_histogram(binwidth = 20, aes(x=onset),data=PicWords_onsets_trial)+
  facet_wrap(~Subject)
```

```{r picture_word_onsets_table}
PicWord_onsets_descr <- aloud_correct %>%
  filter(Stimulus_type == "picture+word") %>%
  summarise(mean_RT = mean(onset), sd_RT = sd(onset))
```

```{r finalise_naming_dataset}
#... filter out the *silent* responses of participant 33
silent %<>% filter(Subject != "AH33")

#And then join the aloud and silent responses.
naming_correct <- full_join(aloud_correct, silent, by = c("Subject", "Dutch_name", "Stimulus_type", "Response_type", "ID", "Probe_type", "English_name", "CorrectName"))
```


## Memory data

```{r read_osf_memory_data}
memory <- rio::import("https://osf.io/48pua/download", "txt")
```

```{r clean_memory_data}
memory %<>% select (Subject, MemoryTrial, ID, Probe_type, Stimulus_type, Response_type, English_name, Dutch_name, Target_button, Response_button, Correct, RT, Repeated) %>% 
  filter(Subject != "AH33")
```

```{r repeated_names}
# find out how many repeated items there were (31) and exclude them (repeated items were incorrectly used in naming but refer to objects used as foils in the memory task)
repeated <- sum(memory$Repeated)

memory %<>% filter(Repeated == 0)
```

```{r memory_accuracy}
#Check how accurate individual participants in the memory task
MemAccuracy_cond <- summarise(group_by(memory, Subject, Stimulus_type, Response_type),
                              proportion_correct = mean(Correct))

ggplot(MemAccuracy_cond)+
  geom_point(aes(x=proportion_correct,y=paste(Response_type,Stimulus_type)))+
  facet_wrap(~Subject)
```

```{r make_final_dataset}
#I am dividing the memory trials into to targets and foils to join them with the naming data. Probably not the best way to do it, but it works.
Memory_targets <- filter(memory, Probe_type == "Target")
Memory_foils <- filter(memory, Probe_type == "Foil")

#First join the targets, i.e. how people did in the naming task with how they responded to the same stims in the memory task. This will get rid of the incorrect naming trials in the memory dataset.
AH_Targets <- inner_join(naming_correct, Memory_targets, by = c("Subject", "Dutch_name", "Stimulus_type", "Response_type", "ID", "Probe_type", "English_name"))

#And then join everything together.
AH_all <- full_join(AH_Targets, Memory_foils, by = c("Subject", "Dutch_name", "Stimulus_type", "Response_type", "ID", "Probe_type", "English_name", "MemoryTrial", "Target_button", "Response_button", "Correct", "RT", "Repeated"))
```

```{r export_analysis_dataset}
write_rds(AH_all, here("Data", "Processed", "AH_all.Rds"))
```


# Analyse data
```{r set_contrasts}
#Set contrasts
AH_all$Probe_type_Base[AH_all$Probe_type == "Foil"] <- -.5
AH_all$Probe_type_Base[AH_all$Probe_type == "Target"] <- .5

AH_all$Stimulus_type_Base[AH_all$Stimulus_type == "picture+word"] <- -.5  
AH_all$Stimulus_type_Base[AH_all$Stimulus_type == "picture"] <- .5  

AH_all$Response_type_Base[AH_all$Response_type == "silent"] <- -.5
AH_all$Response_type_Base[AH_all$Response_type == "aloud"] <- .5

```

```{r base_model}
Logit.0 <- glmer(Response_button ~ Probe_type_Base*Stimulus_type_Base*Response_type_Base +
                  (1 | ID) + (1 + Probe_type_Base+Stimulus_type_Base | Subject),
                 data = AH_all,
                 family = binomial,
                 control = glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=20000)))

summary(Logit.0)
```



