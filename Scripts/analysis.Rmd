---
title: "Example analysis script"
author: "Eirini Zormpa"
date: "09/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
## install what you don't have
# install.packages("here")
# install.packages("readr")
# install.packages("magrittr")
# install.packages("rio")
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages("lme4")

#library(here)
library(readr)
library(magrittr)
library(rio)
library(dplyr)
library(ggplot2)
library(lme4)
```

# Overview of experiment

In this experiment, I was interested to see what word production processes affect memory.
More specifically, I wanted to see if coming up with a word and saying that word aloud affect memory.
I tested this in a two-part experiment, with a study phase and a test phase.

The **study phase** was a picture naming experiment in which participants saw a picture with either the picture name or a gibberish label superimposed and they had to name the picture aloud or silently prompted by a coloured frame.
The first manipulation tests the effect of coming up with a name.
The second manipulation tests the effect of saying that name aloud.

To ensure that participants were naming the pictures in the silent trials the naming phase was split as follows:
* participants saw the pictures without the frames for 800 ms, which should be sufficient for lexical access.
* then the frame appeared around the picture, signalling whether naming should happen aloud or silently
* participants had limited amount of time to name the pictures.

The **test phase** was a Yes/No recognition memory task conducted 20 minutes later.
In this task, participants saw all the pictures from the picture naming task and an equal number of new pictures.
The pictures were presented with the labels and the frames, exactly as in the study phase.
Participants were prompted to say "Yes" to the pictures they remembered from the previous task and "No" to the new pictures.

# Prepare data

The DV here is the response in the memory task.
However, I can't just look at the data from the memory task.
First, I need to remove trials that were named incorrectly and participants that were too slow to name the pictures.
Slow naming could be a sign that those participants were not naming the silent trials.
Information about whether the naming was correct comes from the logfiles of the naming experiment.
Information about the naming times comes from the recording annotations.

## Naming data

First I need to read in my data and have some fun with paths!

To start, I'm pretending I'm not using a project.
I read my file using the entire path.
This works, but of course no one else has the same path as me.

```{r annotations_read_local}
#naming_onsets_local1 <- read_delim("C:/Users/eirin/Documents/MPI Psycholinguistics/Teaching/KPLS2020/KPLS5/Data/Raw/Naming/AH_NamingOnsets.txt", delim = "\t")
```

Wait, I'd have to do aaaaaaaaaall that typing every single type I want to read or write a file?
Having a convenient working directory means less typing.

```{r annotations_read_local_wd}
#setwd("C:/Users/eirin/Documents/MPI Psycholinguistics/Teaching/KPLS2020/KPLS5")
#naming_onsets_local2 <- read_delim("Data/Raw/Naming/AH_NamingOnsets.txt", delim = "\t")
```

Using a project means I don't need to set my working directory anymore--the working directory is automatically set to be the directory where the .Rproject file is.
The main advantage of not manually setting the working directory is that other people can run your code immediately.

Just having a project does not solve the second problem with the code above.
R Markdown files reset the working directory to be the directory that contains the current script (in this case "Scripts").
Using the "here" package solves that.

```{r annotations_read_local_here}
#naming_onsets_local3 <- read_delim(here("Data", "Raw", "Naming", "AH_NamingOnsets.txt"), delim = "\t")
```

But what if I want to share my script with someone that doens't have a local copy of my data?
Or if I want to make it possible for people to run my code online?
It's possible to get data into R from the Open Science Framework (and other repositories)!

```{r _annotations_read_osf}
naming_onsets <- rio::import("https://osf.io/mvtra/download", "txt")

summary(naming_onsets)
```

The summary above shows why you shouldn't touch your raw data.
I have three different onset columns and **I** can figure out why, but maybe not just anyone who looks at my data.

Now just run everything!

```{r remove_copies}
#rm(naming_onsets_local1)
#rm(naming_onsets_local2)
#rm(naming_onsets_local3)
```


```{r annotations_clean}
naming_onsets %<>%
  select(-offset) %>%
  rename(praat_label = label)
```


```{r annotations_aloud}
# Keep only the aloud items (42*64 = 2688) and get rid of unnecessary lines from praat annotation (time before and after word).
# The silent items have no onset and the two chunks with the condition and comments are not useful to me.
aloud_onsets <- filter(naming_onsets, Response_type == "aloud")  %>%
  filter(as.character(praat_label) == as.character(Dutch_name))
```

```{r annotations_aloud_check}
# Sanity check (same number of aloud trials from all participants)
sanity_check <- aloud_onsets %>%
  group_by(Subject) %>%
  summarise(no_rows = length(Subject))

sanity_check$no_rows
```

```{r naming_logfiles_read}
naming <- rio::import("https://osf.io/3rv5f/download", "txt")

summary(naming)
```

```{r naming_logfiles_clean}
naming %<>% 
  select (Subject, ID, Probe_type, Stimulus_type, Response_type, English_name, Dutch_name, CorrectName) %>%
  arrange(Subject, Dutch_name)
```

```{r naming_accuracy}
nam_acc <- mean(naming$CorrectName)
  
#Check how accurate individual participants are with naming
NamAccuracy_cond <- summarise(group_by(naming, Subject, Stimulus_type, Response_type),
                              mean_correct = mean(CorrectName))

ggplot(NamAccuracy_cond)+
  geom_point(aes(x=mean_correct,y=paste(Response_type,Stimulus_type)))+
  facet_wrap(~Subject)
```

```{r join_naming_datasets}
# To join the timing onsets with the aloud items first separate aloud and silent items from naming data
aloud <- naming %>%
  filter(Response_type == "aloud")

silent <- naming %>%
  filter(Response_type == "silent")

# Filtering out incorrect trials
aloud_all <- inner_join(aloud_onsets, aloud, by = c("Subject", "Dutch_name", "Stimulus_type", "Response_type"))%>%
  select(-onset) %>%
  rename(onset = Real_onset) %>%
  mutate(onset = onset - 800)

aloud_correct <- aloud_all %>% filter(CorrectName == 1)
```

```{r visualise_naming_onsets_picture}
Picture_onsets_sub <- aloud_correct %>%
  filter(Stimulus_type == "picture") %>%
  group_by(Subject) %>%
  summarise(mean_RT=mean(onset)) %>%
  arrange(Subject)

ggplot() +
  geom_histogram(binwidth = 15, aes(x=mean_RT),data=Picture_onsets_sub)

Picture_onsets_trial <- aloud_correct %>%
  filter(Stimulus_type == "picture")

ggplot() +
  geom_histogram(binwidth = 20, aes(x=onset),data=Picture_onsets_trial)+
  facet_wrap(~Subject)
```

```{r check_outliers_picture}
# Participant 33 seems slower than the rest with a right-tailed distribution and is excluded
lat_33 <- aloud_correct %>% filter(Subject=="AH33")

mean_33 <- summarise(lat_33, mean_RT = mean(onset), sd_RT = sd(onset))

# exclude participant 33
aloud_correct %<>% filter(Subject != "AH33")
```

```{r picture_onsets_table}
Picture_onsets_descr <- aloud_correct %>%
  filter(Stimulus_type == "picture") %>%
  summarise(mean_RT = mean(onset),
            sd_RT = sd(onset))
```

```{r visualise_naming_onsets_picture_word}
PicWord_onsets_sub <- aloud_correct %>%
  filter(Stimulus_type == "picture+word") %>%
  group_by(Subject) %>%
  arrange(Subject)

ggplot() +
  geom_histogram(binwidth = 15, aes(x=onset),data=PicWord_onsets_sub)

PicWords_onsets_trial <- aloud_correct %>%
  filter(Stimulus_type == "picture+word")

ggplot() +
  geom_histogram(binwidth = 20, aes(x=onset),data=PicWords_onsets_trial)+
  facet_wrap(~Subject)
```

```{r picture_word_onsets_table}
PicWord_onsets_descr <- aloud_correct %>%
  filter(Stimulus_type == "picture+word") %>%
  summarise(mean_RT = mean(onset),
            sd_RT = sd(onset))
```

```{r finalise_naming_dataset}
#... filter out the *silent* responses of participant 33
silent %<>% filter(Subject != "AH33")

#And then join the aloud and silent responses.
naming_correct <- full_join(aloud_correct, silent, by = c("Subject", "Dutch_name", "Stimulus_type", "Response_type", "ID", "Probe_type", "English_name", "CorrectName"))
```


## Memory data

```{r memory_data_read}
memory <- rio::import("https://osf.io/48pua/download", "txt")
```

```{r memory_data_clean}
memory %<>% select (Subject, MemoryTrial, ID, Probe_type, Stimulus_type, Response_type, English_name, Dutch_name, Target_button, Response_button, Correct, RT, Repeated) %>% 
  filter(Subject != "AH33")
```

```{r repeated_names}
# find out how many repeated items there were (31) and exclude them (repeated items were incorrectly used in naming but refer to objects used as foils in the memory task)
repeated <- sum(memory$Repeated)

memory %<>% filter(Repeated == 0)
```

```{r memory_accuracy}
#Check how accurate individual participants in the memory task
MemAccuracy_cond <- summarise(group_by(memory, Subject, Stimulus_type, Response_type),
                              proportion_correct = mean(Correct))

ggplot(MemAccuracy_cond)+
  geom_point(aes(x=proportion_correct,y=paste(Response_type,Stimulus_type)))+
  facet_wrap(~Subject)
```

```{r make_final_dataset}
#I am dividing the memory trials into to targets and foils to join them with the naming data. Probably not the best way to do it, but it works.
Memory_targets <- filter(memory, Probe_type == "Target")
Memory_foils <- filter(memory, Probe_type == "Foil")

#First join the targets, i.e. how people did in the naming task with how they responded to the same stims in the memory task. This will get rid of the incorrect naming trials in the memory dataset.
AH_Targets <- inner_join(naming_correct, Memory_targets, by = c("Subject", "Dutch_name", "Stimulus_type", "Response_type", "ID", "Probe_type", "English_name"))

#And then join everything together.
AH_all <- full_join(AH_Targets, Memory_foils, by = c("Subject", "Dutch_name", "Stimulus_type", "Response_type", "ID", "Probe_type", "English_name", "MemoryTrial", "Target_button", "Response_button", "Correct", "RT", "Repeated"))
```

```{r export_analysis_dataset}
write_rds(AH_all, here("Data", "Processed", "AH_all.Rds"))
```


# Analyse data

```{r set_contrasts}
#Set contrasts
AH_all$Probe_type_Base[AH_all$Probe_type == "Foil"] <- -.5
AH_all$Probe_type_Base[AH_all$Probe_type == "Target"] <- .5

AH_all$Stimulus_type_Base[AH_all$Stimulus_type == "picture+word"] <- -.5  
AH_all$Stimulus_type_Base[AH_all$Stimulus_type == "picture"] <- .5  

AH_all$Response_type_Base[AH_all$Response_type == "silent"] <- -.5
AH_all$Response_type_Base[AH_all$Response_type == "aloud"] <- .5

```

Run the model and check against the published article!

```{r base_model}
Logit.0 <- glmer(Response_button ~ Probe_type_Base*Stimulus_type_Base*Response_type_Base +
                  (1 | ID) + (1 + Probe_type_Base+Stimulus_type_Base | Subject),
                 data = AH_all,
                 family = binomial,
                 control = glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=20000)))

summary(Logit.0)
```

